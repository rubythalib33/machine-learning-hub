{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaVjitkGOAegI/d8WIGX64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubythalib33/machine-learning-hub/blob/main/Reinforcement%20Learning/PPO_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQz5fFBR537_",
        "outputId": "3d11c0e3-fc45-4eed-f7ac-ed561a3b1b43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fWypxlZX0twb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "a8N9kcUw05Wz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OqUc7B_B07Hd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is from openai baseline\n",
        "# https://github.com/openai/baselines/tree/master/baselines/common/vec_env\n",
        "\n",
        "import numpy as np\n",
        "from multiprocessing import Process, Pipe\n",
        "\n",
        "def worker(remote, parent_remote, env_fn_wrapper):\n",
        "    parent_remote.close()\n",
        "    env = env_fn_wrapper.x()\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'step':\n",
        "            ob, reward, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, reward, done, info))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'reset_task':\n",
        "            ob = env.reset_task()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'close':\n",
        "            remote.close()\n",
        "            break\n",
        "        elif cmd == 'get_spaces':\n",
        "            remote.send((env.observation_space, env.action_space))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class VecEnv(object):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a tuple of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environments' resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "    \n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "        \n",
        "class SubprocVecEnv(VecEnv):\n",
        "    def __init__(self, env_fns, spaces=None):\n",
        "        \"\"\"\n",
        "        envs: list of gym environments to run in subprocesses\n",
        "        \"\"\"\n",
        "        self.waiting = False\n",
        "        self.closed = False\n",
        "        nenvs = len(env_fns)\n",
        "        self.nenvs = nenvs\n",
        "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
        "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
        "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
        "        for p in self.ps:\n",
        "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
        "            p.start()\n",
        "        for remote in self.work_remotes:\n",
        "            remote.close()\n",
        "\n",
        "        self.remotes[0].send(('get_spaces', None))\n",
        "        observation_space, action_space = self.remotes[0].recv()\n",
        "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        for remote, action in zip(self.remotes, actions):\n",
        "            remote.send(('step', action))\n",
        "        self.waiting = True\n",
        "\n",
        "    def step_wait(self):\n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        self.waiting = False\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def reset_task(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset_task', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.waiting:\n",
        "            for remote in self.remotes:            \n",
        "                remote.recv()\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('close', None))\n",
        "        for p in self.ps:\n",
        "            p.join()\n",
        "            self.closed = True\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.nenvs"
      ],
      "metadata": {
        "id": "UHumxgUV2IL_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_envs = 16\n",
        "env_name = \"Pendulum-v1\"\n",
        "\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    env = gym.make(env_name)\n",
        "    return env\n",
        "  \n",
        "  return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgrEEtOO1ENA",
        "outputId": "785fc801-b043-4e84-963a-a499b5841103"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)"
      ],
      "metadata": {
        "id": "KBQsd1b81muL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    \n",
        "    self.critic = nn.Sequential(\n",
        "        nn.Linear(num_inputs, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1)\n",
        "    )\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "        nn.Linear(num_inputs, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, num_outputs)\n",
        "    )\n",
        "    self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
        "\n",
        "    self.apply(init_weights)\n",
        "\n",
        "  def forward(self, x):\n",
        "    value = self.critic(x)\n",
        "    mu = self.actor(x)\n",
        "    std = self.log_std.exp().expand_as(mu)\n",
        "    dist = Normal(mu, std)\n",
        "\n",
        "    return dist, value"
      ],
      "metadata": {
        "id": "EO3MPjoX2ZU6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, vis=True):\n",
        "  state = env.reset()\n",
        "  if vis: env.render()\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "  while not done:\n",
        "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "    dist, _ = model(state)\n",
        "    next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "    state = next_state\n",
        "    if vis: env.render()\n",
        "    total_reward += reward\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "cmZPnRw83282"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAE"
      ],
      "metadata": {
        "id": "gbIaDigr4u0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "metadata": {
        "id": "6V7ozPIz4s5y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Proximal Policy Optimization Algorithm"
      ],
      "metadata": {
        "id": "GIyCBde643Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "        \n",
        "        \n",
        "\n",
        "def ppo_update(model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "A3kFEfgf42Sd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = -200\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "U5phXT2s5ETU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frames = 15000\n",
        "frame_idx  = 0\n",
        "test_rewards = []"
      ],
      "metadata": {
        "id": "PBkhSjBe5JdK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = envs.reset()\n",
        "early_stop = False\n",
        "\n",
        "while frame_idx < max_frames and not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    entropy = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy += dist.entropy().mean()\n",
        "        \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "        \n",
        "        if frame_idx % 1000 == 0:\n",
        "            test_reward = np.mean([test_env(model, False) for _ in range(10)])\n",
        "            test_rewards.append(test_reward)\n",
        "            plot(frame_idx, test_rewards)\n",
        "            if test_reward > threshold_reward: early_stop = True\n",
        "            \n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states)\n",
        "    actions   = torch.cat(actions)\n",
        "    advantage = returns - values\n",
        "    \n",
        "    ppo_update(model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "MF1p6Pag5LT1",
        "outputId": "4a8e884d-4520-437a-e651-c04f2cb6ec37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "    \n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "    \n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "        \n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXwQsjaS5nvj",
        "outputId": "7affdfe6-ff8f-47a2-80c8-fa70047c2fb1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0 reward: -1504.7394493927188\n",
            "episode: 1 reward: -1447.2409502277048\n",
            "episode: 2 reward: -1521.336910792674\n",
            "episode: 3 reward: -1440.4739874474678\n",
            "episode: 4 reward: -1407.145554649016\n",
            "episode: 5 reward: -1400.4571585736157\n",
            "episode: 6 reward: -1386.912723402679\n",
            "episode: 7 reward: -1388.9635123793769\n",
            "episode: 8 reward: -1429.7896707690586\n",
            "episode: 9 reward: -1408.8175928736312\n",
            "episode: 10 reward: -1376.7306057099113\n",
            "episode: 11 reward: -1512.1735971764933\n",
            "episode: 12 reward: -1389.1483852600772\n",
            "episode: 13 reward: -1433.1236223864498\n",
            "episode: 14 reward: -1392.6073619031167\n",
            "episode: 15 reward: -1529.9447054529765\n",
            "episode: 16 reward: -1409.403788298406\n",
            "episode: 17 reward: -1346.4298112838912\n",
            "episode: 18 reward: -1405.7618718073197\n",
            "episode: 19 reward: -1396.6400185242535\n",
            "episode: 20 reward: -1418.8658165338798\n",
            "episode: 21 reward: -1478.1725197017486\n",
            "episode: 22 reward: -1418.3280206474774\n",
            "episode: 23 reward: -1441.921160037064\n",
            "episode: 24 reward: -1429.1629106025691\n",
            "episode: 25 reward: -1435.820626424635\n",
            "episode: 26 reward: -1463.883500901437\n",
            "episode: 27 reward: -1494.9118862043217\n",
            "episode: 28 reward: -1447.7114449851736\n",
            "episode: 29 reward: -1430.1245213103655\n",
            "episode: 30 reward: -1517.0204598115101\n",
            "episode: 31 reward: -1413.6879169362724\n",
            "episode: 32 reward: -1521.858129468176\n",
            "episode: 33 reward: -1386.9418316619608\n",
            "episode: 34 reward: -1533.004102551035\n",
            "episode: 35 reward: -1303.6858870000688\n",
            "episode: 36 reward: -1415.9198445513566\n",
            "episode: 37 reward: -1420.598862591159\n",
            "episode: 38 reward: -1443.3472285248824\n",
            "episode: 39 reward: -1398.6130153723652\n",
            "episode: 40 reward: -1402.0210211814683\n",
            "episode: 41 reward: -1422.559622094594\n",
            "episode: 42 reward: -1516.7505747676812\n",
            "episode: 43 reward: -1319.7272871988962\n",
            "episode: 44 reward: -1428.9012288710578\n",
            "episode: 45 reward: -1386.1029587863181\n",
            "episode: 46 reward: -1495.6370879124704\n",
            "episode: 47 reward: -1387.48912749145\n",
            "episode: 48 reward: -1382.9340570063996\n",
            "episode: 49 reward: -1512.5469543039032\n",
            "episode: 50 reward: -1365.0394253056713\n",
            "episode: 51 reward: -1432.8974069806134\n",
            "episode: 52 reward: -1448.0197263144207\n",
            "episode: 53 reward: -1412.677872726418\n",
            "episode: 54 reward: -1461.189980886076\n",
            "episode: 55 reward: -1377.8523452174798\n",
            "episode: 56 reward: -1412.9952057284115\n",
            "episode: 57 reward: -1437.352590776803\n",
            "episode: 58 reward: -1374.0040710695744\n",
            "episode: 59 reward: -1529.7961667065597\n",
            "episode: 60 reward: -1495.4744152884941\n",
            "episode: 61 reward: -1421.8329281126826\n",
            "episode: 62 reward: -1369.3211659255248\n",
            "episode: 63 reward: -1416.7175984445605\n",
            "episode: 64 reward: -1448.729542643803\n",
            "episode: 65 reward: -1353.7978971362238\n",
            "episode: 66 reward: -1410.8121936752764\n",
            "episode: 67 reward: -1411.0801578867927\n",
            "episode: 68 reward: -1379.6540213523288\n",
            "episode: 69 reward: -1449.9171640751233\n",
            "episode: 70 reward: -1412.1010498838768\n",
            "episode: 71 reward: -1426.6391541349772\n",
            "episode: 72 reward: -1361.1905112986376\n",
            "episode: 73 reward: -1404.9753810435516\n",
            "episode: 74 reward: -1394.5781053488483\n",
            "episode: 75 reward: -1479.810966332477\n",
            "episode: 76 reward: -1369.8507687090505\n",
            "episode: 77 reward: -1388.3671808792385\n",
            "episode: 78 reward: -1459.4076887798713\n",
            "episode: 79 reward: -1429.239719621261\n",
            "episode: 80 reward: -1271.5287689539712\n",
            "episode: 81 reward: -1434.7975454086163\n",
            "episode: 82 reward: -1442.4416405546472\n",
            "episode: 83 reward: -1409.5876522867197\n",
            "episode: 84 reward: -1464.2169513189851\n",
            "episode: 85 reward: -1406.7949892522056\n",
            "episode: 86 reward: -1505.0014802851163\n",
            "episode: 87 reward: -1503.5176325855787\n",
            "episode: 88 reward: -1371.4194062242368\n",
            "episode: 89 reward: -1270.5637674288469\n",
            "episode: 90 reward: -1536.8435994813196\n",
            "episode: 91 reward: -1518.0760530384844\n",
            "episode: 92 reward: -1349.2561721100208\n",
            "episode: 93 reward: -1499.7917694679652\n",
            "episode: 94 reward: -1476.0543852117773\n",
            "episode: 95 reward: -1406.8119106077534\n",
            "episode: 96 reward: -1499.0353793251027\n",
            "episode: 97 reward: -1220.928468325235\n",
            "episode: 98 reward: -1451.670757741\n",
            "episode: 99 reward: -1333.2900296485561\n",
            "episode: 100 reward: -1410.5895042246432\n",
            "episode: 101 reward: -1506.0283647941642\n",
            "episode: 102 reward: -1393.885762672111\n",
            "episode: 103 reward: -1475.6750562467237\n",
            "episode: 104 reward: -1376.066760276669\n",
            "episode: 105 reward: -1356.3147045575952\n",
            "episode: 106 reward: -1401.2024847197918\n",
            "episode: 107 reward: -1417.2191563338897\n",
            "episode: 108 reward: -1414.4021436174871\n",
            "episode: 109 reward: -1390.575963349085\n",
            "episode: 110 reward: -1481.1516857349307\n",
            "episode: 111 reward: -1410.0733867278846\n",
            "episode: 112 reward: -1232.8531437526037\n",
            "episode: 113 reward: -1484.6788056936941\n",
            "episode: 114 reward: -1513.251001042811\n",
            "episode: 115 reward: -1408.9865458614113\n",
            "episode: 116 reward: -1511.1092326112675\n",
            "episode: 117 reward: -1262.1615751825639\n",
            "episode: 118 reward: -1505.1983196488145\n",
            "episode: 119 reward: -1477.9666899704166\n",
            "episode: 120 reward: -1381.330414637246\n",
            "episode: 121 reward: -1408.094790039708\n",
            "episode: 122 reward: -1536.4474682083335\n",
            "episode: 123 reward: -1514.4717094764767\n",
            "episode: 124 reward: -1388.005869765533\n",
            "episode: 125 reward: -1481.5211256125574\n",
            "episode: 126 reward: -1404.0099959150646\n",
            "episode: 127 reward: -1411.829535432928\n",
            "episode: 128 reward: -1418.5600823381378\n",
            "episode: 129 reward: -1418.4201075201954\n",
            "episode: 130 reward: -1414.6620360561635\n",
            "episode: 131 reward: -1393.0231112400336\n",
            "episode: 132 reward: -1458.2678571855663\n",
            "episode: 133 reward: -1413.6033424325008\n",
            "episode: 134 reward: -1397.9195215271545\n",
            "episode: 135 reward: -1488.3349254028055\n",
            "episode: 136 reward: -1403.4820252012707\n",
            "episode: 137 reward: -1411.6150688018568\n",
            "episode: 138 reward: -1260.2231868817275\n",
            "episode: 139 reward: -1411.684713835173\n",
            "episode: 140 reward: -1465.630099352294\n",
            "episode: 141 reward: -1384.0397916107165\n",
            "episode: 142 reward: -1510.0650343233985\n",
            "episode: 143 reward: -1401.1274338600754\n",
            "episode: 144 reward: -1420.861944574825\n",
            "episode: 145 reward: -1217.7560138567537\n",
            "episode: 146 reward: -1421.5986631717585\n",
            "episode: 147 reward: -1436.1929011136538\n",
            "episode: 148 reward: -1451.5482155792947\n",
            "episode: 149 reward: -1366.075303218375\n",
            "episode: 150 reward: -1345.3249771809415\n",
            "episode: 151 reward: -1389.4106067059508\n",
            "episode: 152 reward: -1390.0313726627444\n",
            "episode: 153 reward: -1420.9564337781121\n",
            "episode: 154 reward: -1389.2018993550873\n",
            "episode: 155 reward: -1343.606286951459\n",
            "episode: 156 reward: -1346.8315007931099\n",
            "episode: 157 reward: -1460.6252963152547\n",
            "episode: 158 reward: -1474.6044784688354\n",
            "episode: 159 reward: -1451.7803773176488\n",
            "episode: 160 reward: -1357.9664552095992\n",
            "episode: 161 reward: -1324.9799496174462\n",
            "episode: 162 reward: -1358.8257987908173\n",
            "episode: 163 reward: -1364.792333107931\n",
            "episode: 164 reward: -1394.5318921355954\n",
            "episode: 165 reward: -1482.653833673961\n",
            "episode: 166 reward: -1507.9058091893714\n",
            "episode: 167 reward: -1501.830138841875\n",
            "episode: 168 reward: -1499.3630898324186\n",
            "episode: 169 reward: -1530.6128662200897\n",
            "episode: 170 reward: -1420.927445229215\n",
            "episode: 171 reward: -1350.6825981873117\n",
            "episode: 172 reward: -1510.9229827578106\n",
            "episode: 173 reward: -1417.4623564117935\n",
            "episode: 174 reward: -1512.1275513342537\n",
            "episode: 175 reward: -1466.0329813704402\n",
            "episode: 176 reward: -1519.7685781970806\n",
            "episode: 177 reward: -1446.27162191778\n",
            "episode: 178 reward: -1338.531557010502\n",
            "episode: 179 reward: -1266.8445310768022\n",
            "episode: 180 reward: -1408.6377419696678\n",
            "episode: 181 reward: -1378.0194986198196\n",
            "episode: 182 reward: -1285.959271915256\n",
            "episode: 183 reward: -1417.1722262125163\n",
            "episode: 184 reward: -1275.1496713954486\n",
            "episode: 185 reward: -1500.3819548466724\n",
            "episode: 186 reward: -1380.2268091193444\n",
            "episode: 187 reward: -1422.890781414565\n",
            "episode: 188 reward: -1094.2095249590752\n",
            "episode: 189 reward: -1402.2499744678148\n",
            "episode: 190 reward: -1431.6921955448918\n",
            "episode: 191 reward: -1396.7928959735127\n",
            "episode: 192 reward: -1543.77465387314\n",
            "episode: 193 reward: -1499.5084858426687\n",
            "episode: 194 reward: -1441.52239252714\n",
            "episode: 195 reward: -1446.0091927047608\n",
            "episode: 196 reward: -1270.9452231168145\n",
            "episode: 197 reward: -1414.4558898734279\n",
            "episode: 198 reward: -1458.9712375580211\n",
            "episode: 199 reward: -1414.1971448201714\n",
            "episode: 200 reward: -1411.689900954323\n",
            "episode: 201 reward: -1415.0673969923173\n",
            "episode: 202 reward: -1426.856832277511\n",
            "episode: 203 reward: -1391.9946838173298\n",
            "episode: 204 reward: -1442.056928872283\n",
            "episode: 205 reward: -1372.8033186262\n",
            "episode: 206 reward: -1405.7598007076958\n",
            "episode: 207 reward: -1395.313432842303\n",
            "episode: 208 reward: -1418.5307383283318\n",
            "episode: 209 reward: -1334.4872744087945\n",
            "episode: 210 reward: -1279.5815846829234\n",
            "episode: 211 reward: -1360.1982265590204\n",
            "episode: 212 reward: -1464.561326978586\n",
            "episode: 213 reward: -1407.6482972212189\n",
            "episode: 214 reward: -1481.1717396636375\n",
            "episode: 215 reward: -1517.2525891554817\n",
            "episode: 216 reward: -1468.272311359503\n",
            "episode: 217 reward: -1424.5435388572055\n",
            "episode: 218 reward: -1469.33403200476\n",
            "episode: 219 reward: -1384.8256777374027\n",
            "episode: 220 reward: -1298.2409531390574\n",
            "episode: 221 reward: -1462.362794094238\n",
            "episode: 222 reward: -1413.8046000204865\n",
            "episode: 223 reward: -1405.0285784476982\n",
            "episode: 224 reward: -1406.882541257505\n",
            "episode: 225 reward: -1506.5404327420767\n",
            "episode: 226 reward: -1264.4613379221933\n",
            "episode: 227 reward: -1406.253786475182\n",
            "episode: 228 reward: -1410.970155732718\n",
            "episode: 229 reward: -1440.9484429133167\n",
            "episode: 230 reward: -1343.0221211360504\n",
            "episode: 231 reward: -1473.2038844283893\n",
            "episode: 232 reward: -1403.6857109971106\n",
            "episode: 233 reward: -1444.384454632307\n",
            "episode: 234 reward: -1462.7843349716982\n",
            "episode: 235 reward: -1425.1065844261382\n",
            "episode: 236 reward: -1411.5125315780367\n",
            "episode: 237 reward: -1531.3764459062033\n",
            "episode: 238 reward: -1397.5506863050175\n",
            "episode: 239 reward: -1449.423566959498\n",
            "episode: 240 reward: -1512.5416310136054\n",
            "episode: 241 reward: -1477.0780338432676\n",
            "episode: 242 reward: -1432.6197465277487\n",
            "episode: 243 reward: -1524.9330767522567\n",
            "episode: 244 reward: -1439.2850148179884\n",
            "episode: 245 reward: -1530.065327278314\n",
            "episode: 246 reward: -1373.788977264104\n",
            "episode: 247 reward: -1430.1971378274393\n",
            "episode: 248 reward: -1429.9610880022985\n",
            "episode: 249 reward: -1391.3935803359507\n",
            "\n",
            "(50000, 4)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLzlZ-He6g4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}